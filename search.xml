<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>改善神经网络：超参数调试、正则化以及优化</title>
      <link href="/2022/03/16/gai-shan-shen-jing-wang-luo-chao-can-shu-diao-shi-zheng-ze-hua-yi-ji-you-hua/"/>
      <url>/2022/03/16/gai-shan-shen-jing-wang-luo-chao-can-shu-diao-shi-zheng-ze-hua-yi-ji-you-hua/</url>
      
        <content type="html"><![CDATA[<h1 id="一、初始化参数"><a href="#一、初始化参数" class="headerlink" title="一、初始化参数"></a>一、初始化参数</h1><p>对一个三层神经网络（neural network)进行参数初始化,神经网络两个隐藏层的激活函数为Relu函数，输出层为Sigmoid函数。分别采用三种不同的方式，第一种是将初始化参数全部设置为0，第二种是随机生成初始化参数，第三种是采用He等的方法进行参数初始化。</p><h2 id="数据集图像"><a href="#数据集图像" class="headerlink" title="数据集图像"></a>数据集图像</h2><p><img src="https://user-images.githubusercontent.com/100786410/158545729-d576ef7d-33cb-481a-92b6-e4cc836360b8.png"></p><p>此分类器的作用是从红点中分离处蓝点</p><h2 id="1-初始化参数为0（Zeros-initialization-–-setting-initialization-x3D-“zeros”-in-the-input-argument）"><a href="#1-初始化参数为0（Zeros-initialization-–-setting-initialization-x3D-“zeros”-in-the-input-argument）" class="headerlink" title="1.初始化参数为0（Zeros initialization – setting initialization = “zeros” in the input argument）"></a>1.初始化参数为0（Zeros initialization – setting initialization = “zeros” in the input argument）</h2><h3 id="代码"><a href="#代码" class="headerlink" title="代码"></a>代码</h3><figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">def initialize_parameters_zeros(layers_dims):</span><br><span class="line">    parameters = {}</span><br><span class="line">    L = len(layers_dims)</span><br><span class="line">    for l in range(1, L):</span><br><span class="line">        parameters['W'+str(l)]=np.zeros((layers_dims[l],layers_dims[l-1]))</span><br><span class="line">        parameters['b'+str(l)]=np.zeros((layers_dims[l],1))</span><br><span class="line">    return parameters</span><br></pre></td></tr></tbody></table></figure><h3 id="数据"><a href="#数据" class="headerlink" title="数据"></a>数据</h3><blockquote><p>Cost after iteration 0: 0.6931471805599453</p></blockquote><blockquote><p>Cost after iteration 1000: 0.6931471805599453</p></blockquote><blockquote><p>Cost after iteration 2000: 0.6931471805599453</p></blockquote><blockquote><p>Cost after iteration 3000: 0.6931471805599453</p></blockquote><blockquote><p>Cost after iteration 4000: 0.6931471805599453</p></blockquote><blockquote><p>Cost after iteration 5000: 0.6931471805599453</p></blockquote><blockquote><p>Cost after iteration 6000: 0.6931471805599453</p></blockquote><blockquote><p>Cost after iteration 7000: 0.6931471805599453</p></blockquote><blockquote><p>Cost after iteration 8000: 0.6931471805599453</p></blockquote><blockquote><p>Cost after iteration 9000: 0.6931471805599453</p></blockquote><blockquote><p>Cost after iteration 10000: 0.6931471805599455</p></blockquote><blockquote><p>Cost after iteration 11000: 0.6931471805599453</p></blockquote><blockquote><p>Cost after iteration 12000: 0.6931471805599453</p></blockquote><blockquote><p>Cost after iteration 13000: 0.6931471805599453</p></blockquote><blockquote><p>Cost after iteration 14000: 0.6931471805599453</p></blockquote><blockquote><p>On the train set:<br>Accuracy: 0.5<br>On the test set:<br>Accuracy: 0.5</p></blockquote><h2 id="成本函数变化曲线"><a href="#成本函数变化曲线" class="headerlink" title="成本函数变化曲线"></a>成本函数变化曲线</h2><p><img src="https://user-images.githubusercontent.com/100786410/158544470-eefa2bbd-dd52-443f-85d6-d81ae04b006f.png"></p><h2 id="预测结果及分析"><a href="#预测结果及分析" class="headerlink" title="预测结果及分析"></a>预测结果及分析</h2><p><img src="https://user-images.githubusercontent.com/100786410/158551051-b48e49b2-32cc-4014-8884-172abc8fa1b2.png" alt="image"></p><p><img src="https://user-images.githubusercontent.com/100786410/158547871-74a64ba6-b98f-4417-8bb8-1b0cd599b593.png" alt="image"></p><p>可以看到不管是训练集还是测试集，所有的预测结果都是0，然而这是为什么呢？</p><p><img src="https://user-images.githubusercontent.com/100786410/158547408-9608fde7-fddf-42f9-b44d-0c9beed4545b.png" alt="3"></p><p>假设训练集中只一个样本，如图中方式计算得到y_pred=0.5,此时不论真时的y值为1还是为0，所得损失函数都是0.6931471805599453。因此，权重无法得到调整，所以成本函数变化曲线也为一条直线（并未呈现梯度下降的趋势）。且在预测函数中设置了当y_pred&gt;0.5时，预测值为1，所以预测结果也圈为0。<br>初始化参数为0，使隐藏层个单元之间呈现处对称状态，根据其反向传播计算公式发现，此时单一隐藏层各单元均在做相同的运算，相当于一个像logistic一样的线性分类器。</p><h2 id="2-随机初始化（Random-Initialization）"><a href="#2-随机初始化（Random-Initialization）" class="headerlink" title="2.随机初始化（Random Initialization）"></a>2.随机初始化（Random Initialization）</h2><p>为了打破单一隐藏层单元做同样运算的这种对称模式，采用对参数进行随机初始化。</p><h3 id="代码-1"><a href="#代码-1" class="headerlink" title="代码"></a>代码</h3><figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">def initialize_parameters_random(layers_dims):</span><br><span class="line">    </span><br><span class="line">    np.random.seed(3)               # This seed makes sure your "random" numbers will be the as ours</span><br><span class="line">    parameters = {}</span><br><span class="line">    L = len(layers_dims)            # integer representing the number of layers</span><br><span class="line">    </span><br><span class="line">    for l in range(1, L):</span><br><span class="line">        parameters['W'+str(l)]=np.random.randn(layers_dims[l],layers_dims[l-1])*10                        #乘10以放大初始值，以体现较大初始参数的影响（通常乘0.01）</span><br><span class="line">        parameters['b'+str(l)]=np.zeros((layers_dims[l],1))</span><br><span class="line">    return parameters</span><br></pre></td></tr></tbody></table></figure><h3 id="数据-1"><a href="#数据-1" class="headerlink" title="数据"></a>数据</h3><blockquote><p>Cost after iteration 0: inf</p></blockquote><blockquote><p>Cost after iteration 1000: 0.6247924745506072</p></blockquote><blockquote><p>Cost after iteration 2000: 0.5980258056061102</p></blockquote><blockquote><p>Cost after iteration 3000: 0.5637539062842213</p></blockquote><blockquote><p>Cost after iteration 4000: 0.5501256393526495</p></blockquote><blockquote><p>Cost after iteration 5000: 0.5443826306793814</p></blockquote><blockquote><p>Cost after iteration 6000: 0.5373895855049121</p></blockquote><blockquote><p>Cost after iteration 7000: 0.47157999220550006</p></blockquote><blockquote><p>Cost after iteration 8000: 0.39770475516243037</p></blockquote><blockquote><p>Cost after iteration 9000: 0.3934560146692851</p></blockquote><blockquote><p>Cost after iteration 10000: 0.3920227137490125</p></blockquote><blockquote><p>Cost after iteration 11000: 0.38913700035966736</p></blockquote><blockquote><p>Cost after iteration 12000: 0.3861358766546214</p></blockquote><blockquote><p>Cost after iteration 13000: 0.38497629552893475</p></blockquote><blockquote><p>Cost after iteration 14000: 0.38276694641706693</p></blockquote><blockquote><p>On the train set:<br>Accuracy: 0.83<br>On the test set:<br>Accuracy: 0.86</p></blockquote><p>inf代表正无穷（<strong>刚开始时，由于将初始权重值取很大，所以导致a^[3]的值很接近0或1，而一旦预测出错就会导致成本函数很大，例如y=1,而y_pred却接近于0时，loss function=-log(y_pred)，其值同样趋近于无穷</strong>），-inf代表负无穷，<br>从数据中可以看出准确率有了提升</p><h2 id="成本函数变化曲线-1"><a href="#成本函数变化曲线-1" class="headerlink" title="成本函数变化曲线"></a>成本函数变化曲线</h2><p><img src="https://user-images.githubusercontent.com/100786410/158581730-77d1f1c9-a951-4918-8e25-9944b20a565c.png" alt="image"></p><p>可以看到图中曲线呈下降趋势</p><h2 id="预测结果及分析-1"><a href="#预测结果及分析-1" class="headerlink" title="预测结果及分析"></a>预测结果及分析</h2><p><img src="https://user-images.githubusercontent.com/100786410/158583211-c8b3a4ff-a703-4b1a-a1f8-33fe66ae3487.png" alt="image"></p><p><img src="https://user-images.githubusercontent.com/100786410/158583147-1cc092ca-14e4-41a9-8b4c-2d2a05557fd2.png" alt="image"></p><p>从图中可以看到由于打破了对称的影响，不论是训练集还是测试机其预测得出的矩阵中元素也不再全为0了。且在点的分布图中也出现了决策边界，虽然还并不是很准确。<strong>但通过更多时间的训练也能达到较好的效果</strong></p><p>不好的初始化会导致梯度<strong>消失/爆炸</strong>，并且也会减缓优化算法的速度。</p><h2 id="3-He初始化（He-Initialization）"><a href="#3-He初始化（He-Initialization）" class="headerlink" title="3.He初始化（He Initialization）"></a>3.He初始化（He Initialization）</h2><p>He初始化是由He et al.在2015年提出，并以作者的姓氏命名。其与”Xavier initialization”非常相似，”Xavier initialization”中权重的缩放因子为sqrt(1./layers_dims[l-1])，而在”He Initialization”中为sqrt(2./layers_dims[l-1])</p><h3 id="代码-2"><a href="#代码-2" class="headerlink" title="代码"></a>代码</h3><figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">def initialize_parameters_he(layers_dims):</span><br><span class="line">    np.random.seed(3)</span><br><span class="line">    parameters = {}</span><br><span class="line">    L = len(layers_dims) - 1 # integer representing the number of layers</span><br><span class="line">     </span><br><span class="line">    for l in range(1, L + 1):</span><br><span class="line">        parameters['W'+str(l)]=np.random.randn(layers_dims[l],layers_dims[l-1])*(np.sqrt(2./layers_dims[l-1]))</span><br><span class="line">        parameters['b'+str(l)]=np.zeros((layers_dims[l],1))</span><br><span class="line">        </span><br><span class="line">    return parameters</span><br></pre></td></tr></tbody></table></figure><h3 id="数据-2"><a href="#数据-2" class="headerlink" title="数据"></a>数据</h3><blockquote><p>Cost after iteration 0: 0.8830537463419761</p></blockquote><blockquote><p>Cost after iteration 1000: 0.6879825919728063</p></blockquote><blockquote><p>Cost after iteration 2000: 0.6751286264523371</p></blockquote><blockquote><p>Cost after iteration 3000: 0.6526117768893805</p></blockquote><blockquote><p>Cost after iteration 4000: 0.6082958970572938</p></blockquote><blockquote><p>Cost after iteration 5000: 0.5304944491717495</p></blockquote><blockquote><p>Cost after iteration 6000: 0.4138645817071794</p></blockquote><blockquote><p>Cost after iteration 7000: 0.3117803464844441</p></blockquote><blockquote><p>Cost after iteration 8000: 0.23696215330322562</p></blockquote><blockquote><p>Cost after iteration 9000: 0.1859728720920684</p></blockquote><blockquote><p>Cost after iteration 10000: 0.15015556280371808</p></blockquote><blockquote><p>Cost after iteration 11000: 0.12325079292273551</p></blockquote><blockquote><p>Cost after iteration 12000: 0.09917746546525937</p></blockquote><blockquote><p>Cost after iteration 13000: 0.08457055954024283</p></blockquote><blockquote><p>Cost after iteration 14000: 0.07357895962677366</p></blockquote><blockquote><p>On the train set:<br>Accuracy: 0.9933333333333333<br>On the test set:<br>Accuracy: 0.96</p></blockquote><p>相比于随机初始化，迭代次数相同时，He初始化所计算出的成本函数会小很多，且准确率也要高很多</p><h3 id="成本函数变化曲线-2"><a href="#成本函数变化曲线-2" class="headerlink" title="成本函数变化曲线"></a>成本函数变化曲线</h3><p><img src="https://user-images.githubusercontent.com/100786410/158590313-13886a37-8533-4f46-a873-a3fe5891b1a0.png" alt="image"></p><h3 id="预测结果及分析-2"><a href="#预测结果及分析-2" class="headerlink" title="预测结果及分析"></a>预测结果及分析</h3><p><img src="https://user-images.githubusercontent.com/100786410/158590590-f8f58161-f022-4916-ac06-93673e0d61fa.png" alt="image"></p><p>可以看到，利用He初始化可以得到很好的决策边界</p><h1 id="正则化"><a href="#正则化" class="headerlink" title="正则化"></a>正则化</h1><p>对于深度学习模型的过拟合(高方差，high variance)的问题，可以采用扩大训练集的方法来解决，但有时想要获得更多的数据集也不是一件易事。这是，可以采用正则化的方法来解决过拟合的问题。<br>此处以踢足球问题为例，图中显示双方球员的位置，请给出蓝色方守门员建议，将球踢到球场某处以便其队员能更好地用头击球。</p><p><img src="https://user-images.githubusercontent.com/100786410/158593623-ebe7ce14-b861-4f98-ab47-8aafcb3e6ffb.png" alt="image"></p><p><img src="https://user-images.githubusercontent.com/100786410/158593755-444f3c32-9457-4769-b01f-ede1f9443e3f.png" alt="image"></p><p>蓝点代表在此处蓝队球员击球，红点代表此处红队球员击球。从图中可以看出，此问题存在噪声问题，但可以通过正则化的方法来避免过拟合，以得到更好的结果。此处将讨论L2 regularization 以及 dropout 方法</p><p>这里同样采用三层神经网络，首先建立一个整体框架，在这个框架中可以通过改变lambd以及keep_prob（代表了每一层中保留单元的概率）的值来控制是否采用L2 regularization或是dropout。</p><figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br></pre></td><td class="code"><pre><span class="line">def model(X, Y, learning_rate = 0.3, num_iterations = 30000, print_cost = True, lambd = 0, keep_prob = 1):</span><br><span class="line">    """</span><br><span class="line">    Implements a three-layer neural network: LINEAR-&gt;RELU-&gt;LINEAR-&gt;RELU-&gt;LINEAR-&gt;SIGMOID.</span><br><span class="line">    </span><br><span class="line">    Arguments:</span><br><span class="line">    X -- input data, of shape (input size, number of examples)</span><br><span class="line">    Y -- true "label" vector (1 for blue dot / 0 for red dot), of shape (output size, number of examples)</span><br><span class="line">    learning_rate -- learning rate of the optimization</span><br><span class="line">    num_iterations -- number of iterations of the optimization loop</span><br><span class="line">    print_cost -- If True, print the cost every 10000 iterations</span><br><span class="line">    lambd -- regularization hyperparameter, scalar</span><br><span class="line">    keep_prob - probability of keeping a neuron active during drop-out, scalar.</span><br><span class="line">    </span><br><span class="line">    Returns:</span><br><span class="line">    parameters -- parameters learned by the model. They can then be used to predict.</span><br><span class="line">    """</span><br><span class="line">        </span><br><span class="line">    grads = {}</span><br><span class="line">    costs = []                            # to keep track of the cost</span><br><span class="line">    m = X.shape[1]                        # number of examples</span><br><span class="line">    layers_dims = [X.shape[0], 20, 3, 1]</span><br><span class="line">    </span><br><span class="line">    # Initialize parameters dictionary.</span><br><span class="line">    parameters = initialize_parameters(layers_dims)</span><br><span class="line"></span><br><span class="line">    # Loop (gradient descent)</span><br><span class="line"></span><br><span class="line">    for i in range(0, num_iterations):</span><br><span class="line"></span><br><span class="line">        # Forward propagation: LINEAR -&gt; RELU -&gt; LINEAR -&gt; RELU -&gt; LINEAR -&gt; SIGMOID.</span><br><span class="line">        if keep_prob == 1:</span><br><span class="line">            a3, cache = forward_propagation(X, parameters)</span><br><span class="line">        elif keep_prob &lt; 1:</span><br><span class="line">            a3, cache = forward_propagation_with_dropout(X, parameters, keep_prob)</span><br><span class="line">        </span><br><span class="line">        # Cost function</span><br><span class="line">        if lambd == 0:</span><br><span class="line">            cost = compute_cost(a3, Y)</span><br><span class="line">        else:</span><br><span class="line">            cost = compute_cost_with_regularization(a3, Y, parameters, lambd)</span><br><span class="line">            </span><br><span class="line">        # Backward propagation.</span><br><span class="line">        assert (lambd == 0 or keep_prob == 1)   # it is possible to use both L2 regularization and dropout, </span><br><span class="line">                                                # but this assignment will only explore one at a time</span><br><span class="line">        if lambd == 0 and keep_prob == 1:</span><br><span class="line">            grads = backward_propagation(X, Y, cache)</span><br><span class="line">        elif lambd != 0:</span><br><span class="line">            grads = backward_propagation_with_regularization(X, Y, cache, lambd)</span><br><span class="line">        elif keep_prob &lt; 1:</span><br><span class="line">            grads = backward_propagation_with_dropout(X, Y, cache, keep_prob)</span><br><span class="line">        </span><br><span class="line">        # Update parameters.</span><br><span class="line">        parameters = update_parameters(parameters, grads, learning_rate)</span><br><span class="line">        </span><br><span class="line">        # Print the loss every 10000 iterations</span><br><span class="line">        if print_cost and i % 10000 == 0:</span><br><span class="line">            print("Cost after iteration {}: {}".format(i, cost))</span><br><span class="line">        if print_cost and i % 1000 == 0:</span><br><span class="line">            costs.append(cost)</span><br><span class="line">    </span><br><span class="line">    # plot the cost</span><br><span class="line">    plt.plot(costs)</span><br><span class="line">    plt.ylabel('cost')</span><br><span class="line">    plt.xlabel('iterations (x1,000)')</span><br><span class="line">    plt.title("Learning rate =" + str(learning_rate))</span><br><span class="line">    plt.show()</span><br><span class="line">    </span><br><span class="line">    return parameters</span><br></pre></td></tr></tbody></table></figure><p>从代码中可以知道L2 reglarization只设计成本函数的计算和反向传播部分，不涉及正向传播过程。而dropout方法从正向传播起便开始作用</p><h3 id="代码-3"><a href="#代码-3" class="headerlink" title="代码"></a>代码</h3><h4 id="L2-regularization"><a href="#L2-regularization" class="headerlink" title="L2 regularization"></a>L2 regularization</h4><figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br></pre></td><td class="code"><pre><span class="line">def compute_cost_with_regularization(A3, Y, parameters, lambd):</span><br><span class="line">    """</span><br><span class="line">    Implement the cost function with L2 regularization. See formula (2) above.</span><br><span class="line">    </span><br><span class="line">    Arguments:</span><br><span class="line">    A3 -- post-activation, output of forward propagation, of shape (output size, number of examples)</span><br><span class="line">    Y -- "true" labels vector, of shape (output size, number of examples)</span><br><span class="line">    parameters -- python dictionary containing parameters of the model</span><br><span class="line">    </span><br><span class="line">    Returns:</span><br><span class="line">    cost - value of the regularized loss function (formula (2))</span><br><span class="line">    """</span><br><span class="line">    m = Y.shape[1]</span><br><span class="line">    W1 = parameters["W1"]</span><br><span class="line">    W2 = parameters["W2"]</span><br><span class="line">    W3 = parameters["W3"]</span><br><span class="line">    </span><br><span class="line">    cross_entropy_cost = compute_cost(A3, Y) # This gives you the cross-entropy part of the cost</span><br><span class="line">    </span><br><span class="line">    #(≈ 1 lines of code)</span><br><span class="line">    # L2_regularization_cost = </span><br><span class="line">    # YOUR CODE STARTS HERE</span><br><span class="line">    L2_regularization_cost=lambd/(2*m)*(np.sum(np.square(W1))+np.sum(np.square(W2))+np.sum(np.square(W3)))</span><br><span class="line">    </span><br><span class="line">    # YOUR CODE ENDS HERE</span><br><span class="line">    </span><br><span class="line">    cost = cross_entropy_cost + L2_regularization_cost</span><br><span class="line">    </span><br><span class="line">    return cost</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def backward_propagation_with_regularization(X, Y, cache, lambd):</span><br><span class="line">    """</span><br><span class="line">    Implements the backward propagation of our baseline model to which we added an L2 regularization.</span><br><span class="line">    </span><br><span class="line">    Arguments:</span><br><span class="line">    X -- input dataset, of shape (input size, number of examples)</span><br><span class="line">    Y -- "true" labels vector, of shape (output size, number of examples)</span><br><span class="line">    cache -- cache output from forward_propagation()</span><br><span class="line">    lambd -- regularization hyperparameter, scalar</span><br><span class="line">    </span><br><span class="line">    Returns:</span><br><span class="line">    gradients -- A dictionary with the gradients with respect to each parameter, activation and pre-activation variables</span><br><span class="line">    """</span><br><span class="line">    </span><br><span class="line">    m = X.shape[1]</span><br><span class="line">    (Z1, A1, W1, b1, Z2, A2, W2, b2, Z3, A3, W3, b3) = cache</span><br><span class="line">    </span><br><span class="line">    dZ3 = A3 - Y</span><br><span class="line">    #(≈ 1 lines of code)</span><br><span class="line">    # dW3 = 1./m * np.dot(dZ3, A2.T) + None</span><br><span class="line">    # YOUR CODE STARTS HERE</span><br><span class="line">    dW3=1./m*np.dot(dZ3,A2.T)+(lambd/m*W3)</span><br><span class="line">    </span><br><span class="line">    # YOUR CODE ENDS HERE</span><br><span class="line">    db3 = 1. / m * np.sum(dZ3, axis=1, keepdims=True)</span><br><span class="line">    </span><br><span class="line">    dA2 = np.dot(W3.T, dZ3)</span><br><span class="line">    dZ2 = np.multiply(dA2, np.int64(A2 &gt; 0))</span><br><span class="line">    #(≈ 1 lines of code)</span><br><span class="line">    # dW2 = 1./m * np.dot(dZ2, A1.T) + None</span><br><span class="line">    # YOUR CODE STARTS HERE</span><br><span class="line">    dW2 = 1./m*np.dot(dZ2,A1.T)+(lambd/m*W2)</span><br><span class="line">    </span><br><span class="line">    # YOUR CODE ENDS HERE</span><br><span class="line">    db2 = 1. / m * np.sum(dZ2, axis=1, keepdims=True)</span><br><span class="line">    </span><br><span class="line">    dA1 = np.dot(W2.T, dZ2)</span><br><span class="line">    dZ1 = np.multiply(dA1, np.int64(A1 &gt; 0))</span><br><span class="line">    #(≈ 1 lines of code)</span><br><span class="line">    # dW1 = 1./m * np.dot(dZ1, X.T) + None</span><br><span class="line">    # YOUR CODE STARTS HERE</span><br><span class="line">    dW1 = 1./m*np.dot(dZ1,X.T)+(lambd/m*W1)</span><br><span class="line">    </span><br><span class="line">    # YOUR CODE ENDS HERE</span><br><span class="line">    db1 = 1. / m * np.sum(dZ1, axis=1, keepdims=True)</span><br><span class="line">    </span><br><span class="line">    gradients = {"dZ3": dZ3, "dW3": dW3, "db3": db3,"dA2": dA2,</span><br><span class="line">                 "dZ2": dZ2, "dW2": dW2, "db2": db2, "dA1": dA1, </span><br><span class="line">                 "dZ1": dZ1, "dW1": dW1, "db1": db1}</span><br><span class="line">    </span><br><span class="line">    return gradients</span><br></pre></td></tr></tbody></table></figure><h4 id="Dropout"><a href="#Dropout" class="headerlink" title="Dropout"></a>Dropout</h4><figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br></pre></td><td class="code"><pre><span class="line">def forward_propagation_with_dropout(X, parameters, keep_prob = 0.5):</span><br><span class="line">    """</span><br><span class="line">    Implements the forward propagation: LINEAR -&gt; RELU + DROPOUT -&gt; LINEAR -&gt; RELU + DROPOUT -&gt; LINEAR -&gt; SIGMOID.</span><br><span class="line">    </span><br><span class="line">    Arguments:</span><br><span class="line">    X -- input dataset, of shape (2, number of examples)</span><br><span class="line">    parameters -- python dictionary containing your parameters "W1", "b1", "W2", "b2", "W3", "b3":</span><br><span class="line">                    W1 -- weight matrix of shape (20, 2)</span><br><span class="line">                    b1 -- bias vector of shape (20, 1)</span><br><span class="line">                    W2 -- weight matrix of shape (3, 20)</span><br><span class="line">                    b2 -- bias vector of shape (3, 1)</span><br><span class="line">                    W3 -- weight matrix of shape (1, 3)</span><br><span class="line">                    b3 -- bias vector of shape (1, 1)</span><br><span class="line">    keep_prob - probability of keeping a neuron active during drop-out, scalar</span><br><span class="line">    </span><br><span class="line">    Returns:</span><br><span class="line">    A3 -- last activation value, output of the forward propagation, of shape (1,1)</span><br><span class="line">    cache -- tuple, information stored for computing the backward propagation</span><br><span class="line">    """</span><br><span class="line">    </span><br><span class="line">    np.random.seed(1)</span><br><span class="line">    </span><br><span class="line">    # retrieve parameters</span><br><span class="line">    W1 = parameters["W1"]</span><br><span class="line">    b1 = parameters["b1"]</span><br><span class="line">    W2 = parameters["W2"]</span><br><span class="line">    b2 = parameters["b2"]</span><br><span class="line">    W3 = parameters["W3"]</span><br><span class="line">    b3 = parameters["b3"]</span><br><span class="line">    </span><br><span class="line">    # LINEAR -&gt; RELU -&gt; LINEAR -&gt; RELU -&gt; LINEAR -&gt; SIGMOID</span><br><span class="line">    Z1 = np.dot(W1, X) + b1</span><br><span class="line">    A1 = relu(Z1)</span><br><span class="line">    #(≈ 4 lines of code)         # Steps 1-4 below correspond to the Steps 1-4 described above. </span><br><span class="line">    # D1 =                                           # Step 1: initialize matrix D1 = np.random.rand(..., ...)</span><br><span class="line">    # D1 =                                           # Step 2: convert entries of D1 to 0 or 1 (using keep_prob as the threshold)</span><br><span class="line">    # A1 =                                           # Step 3: shut down some neurons of A1</span><br><span class="line">    # A1 =                                           # Step 4: scale the value of neurons that haven't been shut down</span><br><span class="line">    # YOUR CODE STARTS HERE</span><br><span class="line">    D1=np.random.rand(W1.shape[0],X.shape[1])</span><br><span class="line">    D1=(D1&lt;keep_prob).astype(int)</span><br><span class="line">    A1=np.multiply(A1,D1)</span><br><span class="line">    A1=A1/keep_prob</span><br><span class="line">    # YOUR CODE ENDS HERE</span><br><span class="line">    Z2 = np.dot(W2, A1) + b2</span><br><span class="line">    A2 = relu(Z2)</span><br><span class="line">    #(≈ 4 lines of code)</span><br><span class="line">    # D2 =                                           # Step 1: initialize matrix D2 = np.random.rand(..., ...)</span><br><span class="line">    # D2 =                                           # Step 2: convert entries of D2 to 0 or 1 (using keep_prob as the threshold)</span><br><span class="line">    # A2 =                                           # Step 3: shut down some neurons of A2</span><br><span class="line">    # A2 =                                           # Step 4: scale the value of neurons that haven't been shut down</span><br><span class="line">    # YOUR CODE STARTS HERE</span><br><span class="line">    D2=np.random.rand(W2.shape[0],X.shape[1])</span><br><span class="line">    D2=(D2&lt;keep_prob).astype(int)</span><br><span class="line">    A2=np.multiply(A2,D2)</span><br><span class="line">    A2=A2/keep_prob</span><br><span class="line">    # YOUR CODE ENDS HERE</span><br><span class="line">    Z3 = np.dot(W3, A2) + b3</span><br><span class="line">    A3 = sigmoid(Z3)</span><br><span class="line">    </span><br><span class="line">    cache = (Z1, D1, A1, W1, b1, Z2, D2, A2, W2, b2, Z3, A3, W3, b3)</span><br><span class="line">    </span><br><span class="line">    return A3, cache</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def backward_propagation_with_dropout(X, Y, cache, keep_prob):</span><br><span class="line">    """</span><br><span class="line">    Implements the backward propagation of our baseline model to which we added dropout.</span><br><span class="line">    </span><br><span class="line">    Arguments:</span><br><span class="line">    X -- input dataset, of shape (2, number of examples)</span><br><span class="line">    Y -- "true" labels vector, of shape (output size, number of examples)</span><br><span class="line">    cache -- cache output from forward_propagation_with_dropout()</span><br><span class="line">    keep_prob - probability of keeping a neuron active during drop-out, scalar</span><br><span class="line">    </span><br><span class="line">    Returns:</span><br><span class="line">    gradients -- A dictionary with the gradients with respect to each parameter, activation and pre-activation variables</span><br><span class="line">    """</span><br><span class="line">    </span><br><span class="line">    m = X.shape[1]</span><br><span class="line">    (Z1, D1, A1, W1, b1, Z2, D2, A2, W2, b2, Z3, A3, W3, b3) = cache</span><br><span class="line">    </span><br><span class="line">    dZ3 = A3 - Y</span><br><span class="line">    dW3 = 1./m * np.dot(dZ3, A2.T)</span><br><span class="line">    db3 = 1./m * np.sum(dZ3, axis=1, keepdims=True)</span><br><span class="line">    dA2 = np.dot(W3.T, dZ3)</span><br><span class="line">    #(≈ 2 lines of code)</span><br><span class="line">    # dA2 =                # Step 1: Apply mask D2 to shut down the same neurons as during the forward propagation</span><br><span class="line">    # dA2 =                # Step 2: Scale the value of neurons that haven't been shut down</span><br><span class="line">    # YOUR CODE STARTS HERE</span><br><span class="line">    dA2=np.multiply(dA2,D2)</span><br><span class="line">    dA2=dA2/keep_prob</span><br><span class="line">    # YOUR CODE ENDS HERE</span><br><span class="line">    dZ2 = np.multiply(dA2, np.int64(A2 &gt; 0))</span><br><span class="line">    dW2 = 1./m * np.dot(dZ2, A1.T)</span><br><span class="line">    db2 = 1./m * np.sum(dZ2, axis=1, keepdims=True)</span><br><span class="line">    </span><br><span class="line">    dA1 = np.dot(W2.T, dZ2)</span><br><span class="line">    #(≈ 2 lines of code)</span><br><span class="line">    # dA1 =                # Step 1: Apply mask D1 to shut down the same neurons as during the forward propagation</span><br><span class="line">    # dA1 =                # Step 2: Scale the value of neurons that haven't been shut down</span><br><span class="line">    # YOUR CODE STARTS HERE</span><br><span class="line">    dA1=np.multiply(dA1,D1)</span><br><span class="line">    dA1=dA1/keep_prob</span><br><span class="line">    # YOUR CODE ENDS HERE</span><br><span class="line">    dZ1 = np.multiply(dA1, np.int64(A1 &gt; 0))</span><br><span class="line">    dW1 = 1./m * np.dot(dZ1, X.T)</span><br><span class="line">    db1 = 1./m * np.sum(dZ1, axis=1, keepdims=True)</span><br><span class="line">    </span><br><span class="line">    gradients = {"dZ3": dZ3, "dW3": dW3, "db3": db3,"dA2": dA2,</span><br><span class="line">                 "dZ2": dZ2, "dW2": dW2, "db2": db2, "dA1": dA1, </span><br><span class="line">                 "dZ1": dZ1, "dW1": dW1, "db1": db1}</span><br><span class="line">    </span><br><span class="line">    return gradients</span><br></pre></td></tr></tbody></table></figure><p>注意dropout在反向传播阶段同样需要/keep_prob</p><h3 id="结果及分析"><a href="#结果及分析" class="headerlink" title="结果及分析"></a>结果及分析</h3><h4 id="不采用正则化"><a href="#不采用正则化" class="headerlink" title="不采用正则化"></a>不采用正则化</h4><p><img src="https://user-images.githubusercontent.com/100786410/158607986-079906d1-527c-4a16-9f30-c1aab747837c.png" alt="image"></p><p><img src="https://user-images.githubusercontent.com/100786410/158608174-c95d473e-c904-41fb-91c8-6db1f713a0f3.png" alt="image"></p><h4 id="L2-regularization-1"><a href="#L2-regularization-1" class="headerlink" title="L2 regularization"></a>L2 regularization</h4><p>lambd=0.7</p><p><img src="https://user-images.githubusercontent.com/100786410/158605029-43b727b8-83ef-48a9-ad8a-3e53807a600a.png" alt="image"></p><p><img src="https://user-images.githubusercontent.com/100786410/158605811-457381e8-6acc-4407-b84d-4c9dbf520e5f.png" alt="image"></p><h4 id="Dropout-1"><a href="#Dropout-1" class="headerlink" title="Dropout"></a>Dropout</h4><p>keep_prob=0.87</p><p><img src="https://user-images.githubusercontent.com/100786410/158606077-01040ee8-5556-4688-9453-0605b8d00a4b.png" alt="image"></p><p><img src="https://user-images.githubusercontent.com/100786410/158606244-4986af45-0c85-4362-a5df-4477552baa96.png" alt="image"></p><p>dropout 不能在测试集中使用，因为在测试阶段我门不希望输出的函数值是随机的，也不用再/=keep_prob，因为在训练集中做过此处理，所以训练集输出值的期望并未减小</p>]]></content>
      
      
      
        <tags>
            
            <tag> deep learning </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>matplotlib.pyplot</title>
      <link href="/2022/03/08/matplotlib-pyplot/"/>
      <url>/2022/03/08/matplotlib-pyplot/</url>
      
        <content type="html"><![CDATA[<h1 id="1-plt-rcParams"><a href="#1-plt-rcParams" class="headerlink" title="1 plt.rcParams()"></a>1 plt.rcParams()</h1><p>matplotlibrc全称为:matplotlib resourse configurations,其中包含了各种matplotlib中可以更改的各种默认属性</p><figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">import matplotlib.pyplot as plt</span><br><span class="line">print(plt.rcParams)#查看参数及其默认值</span><br><span class="line">plt.rcParams['line.linewidth']=2#使用参数字典访问并修改已经加载的配置项</span><br><span class="line">plt.rcParams['line.color']='r'</span><br><span class="line">plt.rc('lines',linewidth=2,color='r')#通过matplotlib.rc()函数传入属性关键字，可同时修改多个参数</span><br></pre></td></tr></tbody></table></figure><p>查看配置文件路径</p><figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">import matplotlib</span><br><span class="line">matplotlib.get_configdir()#获取用户配置路径</span><br><span class="line">matplotlib.matplotlib_fname()#获得目前使用的配置文件的路径</span><br></pre></td></tr></tbody></table></figure><h1 id="2"><a href="#2" class="headerlink" title="2"></a>2</h1>]]></content>
      
      
      
        <tags>
            
            <tag> python </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>开端</title>
      <link href="/2022/03/03/kai-duan/"/>
      <url>/2022/03/03/kai-duan/</url>
      
        <content type="html"><![CDATA[<p><img src="https://user-images.githubusercontent.com/100786410/156950038-aa078815-780d-4f1e-8c29-5ea59e4223c0.png" alt="1"></p>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>Hello World</title>
      <link href="/2022/03/03/hello-world/"/>
      <url>/2022/03/03/hello-world/</url>
      
        <content type="html"><![CDATA[<p>Welcome to <a href="https://hexo.io/">Hexo</a>! This is your very first post. Check <a href="https://hexo.io/docs/">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href="https://hexo.io/docs/troubleshooting.html">troubleshooting</a> or you can ask me on <a href="https://github.com/hexojs/hexo/issues">GitHub</a>.</p><h2 id="Quick-Start"><a href="#Quick-Start" class="headerlink" title="Quick Start"></a>Quick Start</h2><h3 id="Create-a-new-post"><a href="#Create-a-new-post" class="headerlink" title="Create a new post"></a>Create a new post</h3><figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo new <span class="string">"My New Post"</span></span><br></pre></td></tr></tbody></table></figure><p>More info: <a href="https://hexo.io/docs/writing.html">Writing</a></p><h3 id="Run-server"><a href="#Run-server" class="headerlink" title="Run server"></a>Run server</h3><figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo server</span><br></pre></td></tr></tbody></table></figure><p>More info: <a href="https://hexo.io/docs/server.html">Server</a></p><h3 id="Generate-static-files"><a href="#Generate-static-files" class="headerlink" title="Generate static files"></a>Generate static files</h3><figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo generate</span><br></pre></td></tr></tbody></table></figure><p>More info: <a href="https://hexo.io/docs/generating.html">Generating</a></p><h3 id="Deploy-to-remote-sites"><a href="#Deploy-to-remote-sites" class="headerlink" title="Deploy to remote sites"></a>Deploy to remote sites</h3><figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo deploy</span><br></pre></td></tr></tbody></table></figure><p>More info: <a href="https://hexo.io/docs/one-command-deployment.html">Deployment</a></p>]]></content>
      
      
      
    </entry>
    
    
  
  
</search>
